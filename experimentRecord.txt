------------------------------------------------------------------
10/24/2015 - Simplest unigram features on naive bayes

Input: Apple forum posts
Pre-processing: 
- Strip all HTML
- Strip all punctuation (except apostrophes)
- All lower case
- Porter stemming
Feature extraction:
- Unigram counts
Labels: 3 classes
- score <= 0, 
- 0 < score <= mean + stdev
- mean + stdev < score
Learning model: Multinomial naive bayes, default scikit parameters (alpha == 1.0)
Training set performance: Accuracy 65.6% with 0.1% standard deviation
Test set performance: Accuracy 55.6% with 0.5% standard deviation

NEXT STEP: Tune parameters to reduce overfitting

------------------------------------------------------------------
10/24/2015 - Simplest unigram features on SVM

Input: Apple forum posts
Pre-processing: 
- Strip all HTML
- Strip all punctuation (except apostrophes)
- All lower case
- Porter stemming
Feature extraction:
- Unigram counts
Labels: 3 classes
- score <= 0, 
- 0 < score <= mean + stdev
- mean + stdev < score
Learning model: Linear SVM, default scikit parameters (C == 1.0)
Training set performance: Accuracy 81.9% with 0.2% standard deviation
Test set performance: Accuracy 54.7% with 0.8% standard deviation

NEXT STEP: Tune parameters to reduce overfitting

------------------------------------------------------------------
10/24/2015 - Tuning alpha to reduce overfitting for multinomial naive bayes

Input: Apple forum posts
Pre-processing: 
- Strip all HTML
- Strip all punctuation (except apostrophes)
- All lower case
- Porter stemming
Feature extraction:
- Unigram counts
Labels: 3 classes
- score <= 0, 
- 0 < score <= mean + stdev
- mean + stdev < score
Learning model: Multinomial naive bayes, (alpha == 2.0, 5.0, 10.0)
Training set performance: Accuracy 64.9%, 61.2%, 59% respectively (standard deviation ~0.1%)
Test set performance: Accuracy 58.2%, 59%, 58.6% respectively (standard deviation ~0.3%)

NEXT STEP: Improve features to reduce bias, keeping larger alpha value

------------------------------------------------------------------
10/24/2015 - Tuning C to reduce overfitting for SVM

Input: Apple forum posts
Pre-processing: 
- Strip all HTML
- Strip all punctuation (except apostrophes)
- All lower case
- Porter stemming
Feature extraction:
- Unigram counts
Labels: 3 classes
- score <= 0, 
- 0 < score <= mean + stdev
- mean + stdev < score
Learning model: Linear SVM, default scikit parameters (C == .01, .0001)
Training set performance: Accuracy 67.2%, 59.5% respectively (standard deviation ~0.1%)
Test set performance: Accuracy 58.4%, 59% respectively (standard deviation ~0.2%)

NEXT STEP: Improve features to reduce bias, keeping smaller C value

------------------------------------------------------------------
11/12/2015 - New Y variable, 2 new X features, Adding precision/recall measures

Input: Apple forum posts
Pre-processing: 
- Strip all HTML
- Strip all punctuation (except apostrophes)
- All lower case
Feature extraction: total of 89,958 features initially
- Unigram counts
- Indicator of whether image exists
- Cosine similarity between question and answer
Labels: 3 classes (where adjusted score is y - y_hat)
- adjusted score < mean-stdev, 
- mean-stdev <= adjusted score < mean + stdev
- mean + stdev <= adjusted score
Learning model: Multinomial NB (alpha = 1, 2, 5, 10) and Linear SVM (C = 1, 0.01, 0.0001)
Performance (respectively): accuracy, precision, recall on training and test sets
(0.7223216154862183, 0.6408195526474673, [0.5921168834705196, 0.7671328245927903, 0.4458996165179796], [0.2257983768468689, 0.7267920689035987, 0.2708114565710509], [0.19831028324960753, 0.898221434385958, 0.35652971815840406], [0.08812731348711897, 0.8285878670626046, 0.2437296812012128])
(0.7188563161981807, 0.6834711882244241, [0.6118427921832928, 0.7387126482060211, 0.45983061694673966], [0.24461692037208885, 0.7221388037065231, 0.2806558516047115], [0.05673468677446856, 0.9501593555931069, 0.21104086212682177], [0.024135347569030626, 0.9207593136047384, 0.14278154078272987])
(0.7177883763422573, 0.7095751887682058, [0.5306400721208023, 0.7213323761693402, 0.5062178812959607], [0.1872951618032626, 0.7178235414888499, 0.2815325452640297], [0.00566476801843563, 0.9899437064982066, 0.05368147978767142], [0.002025605383974615, 0.9831597138976422, 0.033718778620799486])
(0.7173039426228348, 0.7153662045462073, [0.6093421052631579, 0.7177189053451656, 0.5841766672500537], [0.2333333333333333, 0.7167981801935628, 0.2783894176121184], [0.0011094495130133085, 0.9984519784702801, 0.011894332490767585], [0.0003076375746368291, 0.9970969992282275, 0.005889717144822724])
(0.8947885571261305, 0.6282905337112046, [0.9567803187975118, 0.8792022459691138, 0.9634462770247575], [0.16624756696340176, 0.721743180585347, 0.24448647067883597], [0.5981908288302373, 0.9913343270735936, 0.702953407859703], [0.096580208981597, 0.8235497499699997, 0.1725213452748474])
(0.7494247471457223, 0.7091568460046065, [0.9680414270569457, 0.7424610510791129, 0.9204800336366074], [0.16871255752234615, 0.7200094684991402, 0.36712018848577077], [0.0419613466695216, 0.9973033855838576, 0.20312938174635323], [0.004291502076054271, 0.9759431279501903, 0.06497891297413895])
(0.7183058314481724, 0.7166324199989582, [1.0, 0.7180799110516235, 0.7985057389033349], [0.0, 0.7172139829288816, 0.5004329004329005], [0.00023351811256824804, 0.999362342759817, 0.01522923785582794], [0.0, 0.998325211442116, 0.008920482180387228])

NEXT STEP: Weight class 0 and 2 samples higher, implement feature selection to bring down ratio of data points to features