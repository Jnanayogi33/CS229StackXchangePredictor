------------------------------------------------------------------
10/24/2015 - Simplest unigram features on naive bayes

Input: Apple forum posts
Pre-processing: 
- Strip all HTML
- Strip all punctuation (except apostrophes)
- All lower case
- Porter stemming
Feature extraction:
- Unigram counts
Labels: 3 classes
- score <= 0, 
- 0 < score <= mean + stdev
- mean + stdev < score
Learning model: Multinomial naive bayes, default scikit parameters (alpha == 1.0)
Training set performance: Accuracy 65.6% with 0.1% standard deviation
Test set performance: Accuracy 55.6% with 0.5% standard deviation

NEXT STEP: Tune parameters to reduce overfitting

------------------------------------------------------------------
10/24/2015 - Simplest unigram features on SVM

Input: Apple forum posts
Pre-processing: 
- Strip all HTML
- Strip all punctuation (except apostrophes)
- All lower case
- Porter stemming
Feature extraction:
- Unigram counts
Labels: 3 classes
- score <= 0, 
- 0 < score <= mean + stdev
- mean + stdev < score
Learning model: Linear SVM, default scikit parameters (C == 1.0)
Training set performance: Accuracy 81.9% with 0.2% standard deviation
Test set performance: Accuracy 54.7% with 0.8% standard deviation

NEXT STEP: Tune parameters to reduce overfitting

------------------------------------------------------------------
10/24/2015 - Tuning alpha to reduce overfitting for multinomial naive bayes

Input: Apple forum posts
Pre-processing: 
- Strip all HTML
- Strip all punctuation (except apostrophes)
- All lower case
- Porter stemming
Feature extraction:
- Unigram counts
Labels: 3 classes
- score <= 0, 
- 0 < score <= mean + stdev
- mean + stdev < score
Learning model: Multinomial naive bayes, (alpha == 2.0, 5.0, 10.0)
Training set performance: Accuracy 64.9%, 61.2%, 59% respectively (standard deviation ~0.1%)
Test set performance: Accuracy 58.2%, 59%, 58.6% respectively (standard deviation ~0.3%)

NEXT STEP: Improve features to reduce bias, keeping larger alpha value

------------------------------------------------------------------
10/24/2015 - Tuning C to reduce overfitting for SVM

Input: Apple forum posts
Pre-processing: 
- Strip all HTML
- Strip all punctuation (except apostrophes)
- All lower case
- Porter stemming
Feature extraction:
- Unigram counts
Labels: 3 classes
- score <= 0, 
- 0 < score <= mean + stdev
- mean + stdev < score
Learning model: Linear SVM, default scikit parameters (C == .01, .0001)
Training set performance: Accuracy 67.2%, 59.5% respectively (standard deviation ~0.1%)
Test set performance: Accuracy 58.4%, 59% respectively (standard deviation ~0.2%)

NEXT STEP: Improve features to reduce bias, keeping smaller C value